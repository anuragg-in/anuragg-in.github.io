<h1>Automating Website Content: A Comparison between Client-Side and Server-Side Automation</h1>
<script src="https://cdn.dashjs.org/latest/dash.all.min.js"></script>
<figure>
  <video class='videoPlayer' controls>
    <source src='https://in2eco.com/poi/videos/dash/32/timelapse/output.mpd' type='application/dash+xml' />
  </video>
</figure>
<script>
  // Select all video elements with the class 'videoPlayer'
  var videos = document.querySelectorAll(".videoPlayer");

  // Initialize each video player with dash.js
  videos.forEach(function(video) {
      var player = dashjs.MediaPlayer().create();
      player.initialize(video, null, true);
  });
</script>
<h2>Overview</h2>
<p>This article compares client-side and server-side automation of website content.</p>
<h2>Prerequisite</h2>
<p><strong>Mandatory:</strong> Website design using HTML, CSS, Javascript and PHP.</p>
<p><strong>Flexible:</strong> Basic knowledge of search engines and crawlers.</p>
<!-- ************************ SECTION: APPLICATION ************************ -->
<h2>Search Engines and Crawlers</h2>
<p>A search engine indexes web page and ranks them. Search engines use crawlers to discover new web pages and contents. Crawling web pages on internet is equivalent to traveling in real world. Crawlers are resource-constrained. Therefore, most crawlers only discover static content.</p>
<h2>Automating Website Content on the Client Side</h2>
<p>There are several ways to to automatically generate content for a website on the client side; the most popular being Javascript. Common use cases for client-side automation includes: adjusting CSS property, lazy image loading, and adding content where crawlability by search engines is not important.</p>
<figure>
  <img alt='client_side' src='../images/blog/automating_website_content/clientside.png'/>
  <figcaption>Static website using HTML, CSS and Javascript. All the files are transferred from the server to the client followed by automatic content generation using Javascript. The automated content generated by Javascript is not discoverable by resource-constrained crawlers.</figcaption>
</figure>
<h3>Automating Website Content on the Server Side</h3>
<p>There are several ways to to automatically generate content for a website on the server sider; the most popular being PHP. Server-side automation has superior capabilities compared to client-side automation. Common use cases for server-side automation includes: adding content where crawlability by search engines is not important, customizing content based on stored individual user preference.</p>
<figure>
  <img alt='server_side' src='../images/blog/automating_website_content/serverside.png'/>
  <figcaption>Dynamic website using HTML, CSS, Javascript and PHP. Server-side processing using PHP automates addition of content on the server side before delivering it to the content. The automated content generated by PHP is discoverable by resource-constrained crawlers.</figcaption>
</figure>
<h3>Considerations for Search Engine Discovery: Crawler-Friendly Websites</h3>
<p>Depending on how one automates content on website, one may need additional steps to make the content on their website discoverable by search engines. For client-side automation of website content, pre-rendering of content is required for discovery. Automation can also be used for efficient delivery of content, for e.g. lazy loading of image, and DASH video streaming. These automation prevents media discovery by search engines; one can make the media discoverable as follows:
<ul>
  <li>Use &lt;noscript&gt; tags to show media to crawlers without lazy loading or DASH streaming</li>
  <li>Use meta tags to direct crawlers to a crawler-friendly website.</li>
  <li>For websites designed using HTML, CSS, and Javascript, use pre-rendering to generate crawler-friendly static websites.</li>
</ul>
</p>
